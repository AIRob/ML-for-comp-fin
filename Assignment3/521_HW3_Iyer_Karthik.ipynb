{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Convex low rank matrix factorization\n",
    "Solve using proximal gradient descent the following problem,\n",
    "$$\\min_X \\frac{1}{2}\\|R - X\\|_F^2 + \\lambda\\|X\\|_*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following facts to set up the proximal gradient descent algorithm.\n",
    "\n",
    "1) $\\|X\\|_{*} = \\sum_{i = 1}^n \\sigma_i$; the nuclear norm of X is the sum of singular values of X.\n",
    "\n",
    "2) For $g(X) = ||X||_{*}$, \n",
    "$\\mathrm{prox}_{\\kappa g}(Z) = U \\hat \\Sigma V^T$, where $\\hat \\Sigma$ is a $n \\times n$ diagonal matrix with $i$th diagonal entry equal to $\\hat \\sigma_i$ where \n",
    "$$\n",
    "\\hat \\sigma_i = \n",
    "\\begin{cases}\n",
    " \\sigma_i - \\kappa, \\mbox{ when }\\sigma_i > \\kappa \\\\\n",
    "0  \\mbox{ otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "Here $U$ and $V$ are such that the SVD of $Z = U \\Sigma V^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "from numpy import linalg as LA\n",
    "\n",
    "np.random.seed(123)\n",
    "m = 500\n",
    "n = 200\n",
    "R = np.random.randn(m, n)\n",
    "regularizing_constant = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "# Compute the objective function\n",
    "def cvx_mf_obj(X):\n",
    "    U, s, V = np.linalg.svd(X, full_matrices=False)\n",
    "    #print s\n",
    "    return 0.5*LA.norm(R - X, 'fro')**2 + regularizing_constant*np.sum(s)\n",
    "\n",
    "# Compute the gradient of smooth part of objective functon\n",
    "def cvx_mf_smooth_grad(X):\n",
    "    return (X-R)\n",
    "\n",
    "# Compute the prox of nuclear norm\n",
    "def prox_nuclear_norm(Z, kappa):\n",
    "    U, s, V = np.linalg.svd(Z, full_matrices=False)\n",
    "    prox_vector = np.zeros(len(s))\n",
    "    for i in range(len(s)):\n",
    "        if s[i] > kappa:\n",
    "            prox_vector[i] = s[i] - kappa\n",
    "    return np.dot(np.dot(U, np.diag(prox_vector)), V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0.0, obj is 52415.732739, error is 316.617817\n",
      "iteration number 1.0, obj is 52415.732739, error is 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Proximal gradient method (Q.2, part 3)\n",
    "\n",
    "tol = 1e-6\n",
    "itm = 10000\n",
    "err = float('Inf')\n",
    "X = R\n",
    "X_old = X\n",
    "step_size =  1.0 # step size is  1/Beta and Beta is 1 in our case.\n",
    "\n",
    "for i in range(itm):\n",
    "    X_old = X\n",
    "    \n",
    "    # Update X\n",
    "    g = cvx_mf_smooth_grad(X)\n",
    "    X = prox_nuclear_norm(X - step_size*g, regularizing_constant*step_size)\n",
    "    \n",
    "    #Update convergence history\n",
    "    obj = cvx_mf_obj(X)\n",
    "    err = LA.norm(X - X_old, 'fro')/ step_size\n",
    "    print \"iteration number %.1f, obj is %f, error is %f\" % (i, obj, err)\n",
    "    if err < tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analysis of the result (Q.2, part 4)\n",
    "\n",
    "k = LA.matrix_rank(X) \n",
    "U, s, V  = np.linalg.svd(R, full_matrices=False)\n",
    "U = U[:,:k]\n",
    "V = V[:k,:]\n",
    "s = s[:k]\n",
    "X_k= np.dot(np.dot(U,np.diag(s)), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Nonconvex low rank matrix factorization\n",
    "Consider the low rank variation of the low rank matrix factorization,\n",
    "$$\n",
    "\\min_{B, F} \\frac{1}{2}\\|R - B F\\|_F^2,\\quad \\text{s.t. }B \\in R^{m\\times k},\\, F\\in R^{k \\times n}.\n",
    "$$\n",
    "where $k$ is the rank we obtain from Q.2 Part 3). Use PALM algorithm to solve this optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the objective function\n",
    "\n",
    "def noncvx_mf_obj(B,F):\n",
    "     return 0.5*LA.norm(R - np.dot(B, F), 'fro')**2\n",
    "\n",
    "# Compute the gradient w.r.t B of the objective function\n",
    "def noncvx_mf_grad_B(B,F):\n",
    "    return np.dot(np.dot(B, F) - R, F.T)\n",
    "\n",
    "# Compute the gradient w.r.t F of the objective function\n",
    "def noncvx_mf_grad_F(B,F):\n",
    "    return np.dot(B.T, np.dot(B, F) - R)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0.0, obj is 675337.856440, error is 35.616647\n",
      "iteration number 1000.0, obj is 34728.428063, error is 0.039273\n",
      "iteration number 2000.0, obj is 34547.816949, error is 0.013946\n",
      "iteration number 3000.0, obj is 34515.366496, error is 0.007723\n",
      "iteration number 4000.0, obj is 34501.878548, error is 0.005883\n",
      "iteration number 5000.0, obj is 34493.487429, error is 0.004671\n",
      "iteration number 6000.0, obj is 34488.427610, error is 0.003535\n",
      "iteration number 7000.0, obj is 34485.558344, error is 0.002665\n",
      "iteration number 8000.0, obj is 34483.836572, error is 0.002133\n",
      "iteration number 9000.0, obj is 34482.614441, error is 0.001881\n",
      "iteration number 10000.0, obj is 34481.557282, error is 0.001814\n",
      "iteration number 11000.0, obj is 34480.507371, error is 0.001839\n",
      "iteration number 12000.0, obj is 34479.406929, error is 0.001886\n",
      "iteration number 13000.0, obj is 34478.262782, error is 0.001908\n",
      "iteration number 14000.0, obj is 34477.123326, error is 0.001880\n",
      "iteration number 15000.0, obj is 34476.053549, error is 0.001795\n",
      "iteration number 16000.0, obj is 34475.110001, error is 0.001661\n",
      "iteration number 17000.0, obj is 34474.324756, error is 0.001497\n",
      "iteration number 18000.0, obj is 34473.702835, error is 0.001318\n",
      "iteration number 19000.0, obj is 34473.229384, error is 0.001141\n",
      "iteration number 20000.0, obj is 34472.879643, error is 0.000975\n"
     ]
    }
   ],
   "source": [
    "# PALM algorithm\n",
    "\n",
    "tol = 1e-6\n",
    "itm = 20000\n",
    "err = float('Inf')\n",
    "np.random.seed(123)\n",
    "\n",
    "B = np.random.randn(m,k)\n",
    "B_old = B\n",
    "F = np.random.randn(k,n)\n",
    "F_old = F\n",
    "\n",
    "for i in range(itm +1):\n",
    "    \n",
    "    # update B  \n",
    "    eta_B = 0.0 \n",
    "    beta_B = LA.norm(np.dot(F_old, F_old.T))\n",
    "    eta_B = 1.0/ beta_B\n",
    "    g_B = noncvx_mf_grad_B(B_old ,F_old)\n",
    "    B = B_old - eta_B*g_B\n",
    "    \n",
    "    # update F\n",
    "    eta_F = 0.0\n",
    "    beta_F = LA.norm(np.dot(B.T, B))\n",
    "    eta_F = 1.0/ beta_F\n",
    "    g_F = noncvx_mf_grad_F(B, F_old)\n",
    "    F = F_old - eta_F*g_F\n",
    "\n",
    "    # Update convergence history\n",
    "    err = LA.norm(B - B_old, 'fro') + LA.norm(F - F_old, 'fro')\n",
    "    obj = noncvx_mf_obj(B, F)\n",
    "    B_old = B\n",
    "    F_old = F\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print \"iteration number %.1f, obj is %f, error is %f\" % (i, obj, err)\n",
    "    if err < tol:   \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.067407916261938011"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the result of non convex low rank approximation and convex low rank approximation\n",
    "\n",
    "LA.norm(X_k - np.dot(B, F), 'fro')/ LA.norm(X_k, 'fro')\n",
    "#LA.norm(X_k - np.dot(B, F), 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only about 6% change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Robust Risk Decomposition\n",
    "Consider the problem\n",
    "$$\\min_{B,F} \\rho_\\kappa(\\bar{R} - BF), \\quad\\text{s.t. }B^TB = I.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data and set parameters\n",
    "\n",
    "N   = 499\n",
    "T   = 251\n",
    "fid = open(\"returns.bin\", \"rb\")\n",
    "R = np.fromfile(fid, np.float64)\n",
    "fid.close()\n",
    "R = np.reshape(R, (T, N)).T\n",
    "\n",
    "# Standardize each row of R by subtracting the mean\n",
    "Y = R- R.mean(axis=1, keepdims=True)\n",
    "kappa = 1.0\n",
    "itm = 8000\n",
    "tol = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def huber(r):\n",
    "    val = 0.0;\n",
    "    for row in r:\n",
    "        for elem in row:\n",
    "            a = np.absolute(elem)\n",
    "            if a > kappa:\n",
    "                val += kappa*a - 0.5*kappa**2\n",
    "            else:\n",
    "                val += 0.5*a**2\n",
    "    return val\n",
    "\n",
    "def f_robust(B,F):\n",
    "    return huber(Y - np.dot(B, F))\n",
    "\n",
    "# Compute the Huber derivative and vectorize it \n",
    "# to apply over each element of a matrix.\n",
    "\n",
    "def huber_derivative(x):\n",
    "    if x < -kappa:\n",
    "        return -kappa\n",
    "    elif x > kappa:\n",
    "        return kappa\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "huber_derivative = np.vectorize(huber_derivative)\n",
    "\n",
    "def grad_B(B,F):\n",
    "    r = Y - np.dot(B, F)\n",
    "    return -np.dot(huber_derivative(r), F.T)\n",
    " \n",
    "def grad_F(B,F):\n",
    "    r = Y - np.dot(B, F)\n",
    "    return -np.dot(B.T, huber_derivative(r))\n",
    "\n",
    "def prox_nearestorthonormal(Z):\n",
    "    U, s, V = np.linalg.svd(Z, full_matrices=False)\n",
    "    return np.dot(U, V.T)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  number 0.0, obj is 54921.841992, error is 84.045383\n",
      "iteration  number 500.0, obj is 39107.775804, error is 151.520358\n",
      "iteration  number 1000.0, obj is 38848.455328, error is 147.442987\n",
      "iteration  number 1500.0, obj is 39727.879453, error is 150.308423\n",
      "iteration  number 2000.0, obj is 39729.766647, error is 151.049622\n",
      "iteration  number 2500.0, obj is 40060.451172, error is 151.717607\n",
      "iteration  number 3000.0, obj is 40201.702201, error is 151.897731\n",
      "iteration  number 3500.0, obj is 40047.377317, error is 153.711456\n",
      "iteration  number 4000.0, obj is 40259.764243, error is 149.596692\n",
      "iteration  number 4500.0, obj is 39681.626930, error is 149.364574\n",
      "iteration  number 5000.0, obj is 40706.403203, error is 155.081662\n",
      "iteration  number 5500.0, obj is 39309.258399, error is 150.096454\n",
      "iteration  number 6000.0, obj is 39487.439653, error is 149.488040\n",
      "iteration  number 6500.0, obj is 39938.489721, error is 148.019159\n",
      "iteration  number 7000.0, obj is 39851.280803, error is 149.054242\n",
      "iteration  number 7500.0, obj is 39647.347970, error is 148.779143\n"
     ]
    }
   ],
   "source": [
    "# PALM algorithm\n",
    "\n",
    "itm = 8000\n",
    "tol = 1e-6\n",
    "err = float('Inf')\n",
    "\n",
    "np.random.seed(123)\n",
    "B = np.eye(N, k)\n",
    "B_old = B\n",
    "\n",
    "F = np.random.randn(k,T)\n",
    "F_old  = F\n",
    "\n",
    "for i in range(itm):\n",
    "    \n",
    "    # Update B\n",
    "    eta_B = 0.0 \n",
    "    beta_B = LA.norm(np.dot(F_old, F_old.T))\n",
    "    eta_B = 1.0/ beta_B\n",
    "    gr_B = grad_B(B_old,  F_old)\n",
    "    B = prox_nearestorthonormal(B_old - eta_B*gr_B)\n",
    "    \n",
    "    # Update F. Note that eta_F is always 1 since beta_F = ||B^TB||_2 = 1\n",
    "    eta_F = 1.0\n",
    "    gr_F = grad_F(B, F_old)\n",
    "    F = F_old - eta_F*gr_F\n",
    "    \n",
    "    # Update convergence  history \n",
    "    err = LA.norm(B - B_old, 'fro') + LA.norm(F - F_old, 'fro')\n",
    "    \n",
    "    obj = f_robust(B, F)\n",
    "    B_old = B\n",
    "    F_old = F\n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print \"iteration  number %.1f, obj is %f, error is %f\" % (i, obj, err)\n",
    "    if err < tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = B[:,0]\n",
    "ind = np.argmax(np.absolute(b1))\n",
    "ind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
